{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlcFaYIWVvvB",
        "outputId": "0bb5d943-622f-4599-b7d0-dfc4afbe39ba"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/kab.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-kab.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-kab.pkl\n",
            "[Go] => [Ddu]\n",
            "[Go] => [Ddut]\n",
            "[Go] => [Ddumt]\n",
            "[Go] => [Ruḥ]\n",
            "[Go] => [Ruḥet]\n",
            "[Go] => [Ruḥemt]\n",
            "[Hi] => [Azul ]\n",
            "[Hi] => [Azul]\n",
            "[Run] => [Azzel]\n",
            "[Run] => [Azzlet]\n",
            "[Run] => [Azzlemt]\n",
            "[Run] => [Azzel]\n",
            "[Run] => [Azzelet]\n",
            "[Run] => [Azzelemt]\n",
            "[Who] => [Anwa]\n",
            "[Who] => [Anta]\n",
            "[Wow] => [Waw]\n",
            "[Wow] => [Muqel kan]\n",
            "[Wow] => [Ẓeṛ kan]\n",
            "[Wow] => [Ihuh]\n",
            "[Fire] => [Times]\n",
            "[Fire] => [Jbed]\n",
            "[Fire] => [Qres]\n",
            "[Help] => [Annaɣ ]\n",
            "[Help] => [Abbuh]\n",
            "[Hide] => [Ffer]\n",
            "[Hide] => [Ffret]\n",
            "[Hide] => [Ffremt]\n",
            "[Jump] => [Neggez]\n",
            "[Jump] => [Nteg]\n",
            "[Jump] => [Neggez]\n",
            "[Jump] => [Nṭeg]\n",
            "[Jump] => [Nṭew]\n",
            "[Jump] => [Nteg]\n",
            "[Jump] => [Ǧellbet]\n",
            "[Jump] => [Ǧellbemt]\n",
            "[Jump] => [Ǧelleb]\n",
            "[Stay] => [Qqimemt]\n",
            "[Stay] => [Qqim]\n",
            "[Stay] => [Qqimet]\n",
            "[Stop] => [Ḥbes]\n",
            "[Stop] => [Bed]\n",
            "[Wait] => [Rǧu]\n",
            "[Wait] => [Rǧut]\n",
            "[Wait] => [Rǧumt]\n",
            "[Wait] => [Rju]\n",
            "[Go on] => [Kemmel]\n",
            "[Go on] => [Kemmlemt]\n",
            "[Hello] => [Azul]\n",
            "[Hello] => [Azul]\n",
            "[Hello] => [Ɛelxiṛ]\n",
            "[Hello] => [Axir]\n",
            "[Hello] => [Saḥit]\n",
            "[Hurry] => [Ɣiwel]\n",
            "[I ran] => [Ttazzaleɣ]\n",
            "[I ran] => [Uzzleɣ]\n",
            "[I try] => [Tteɛṛaḍeɣ]\n",
            "[I won] => [Rebḥeɣ]\n",
            "[I won] => [Rniɣ]\n",
            "[I won] => [Rebḥeɣ]\n",
            "[Oh no] => [Ala xaṭi]\n",
            "[Relax] => [Thedden]\n",
            "[Shoot] => [Qres]\n",
            "[Smile] => [Acmumeḥ]\n",
            "[Smile] => [Azmumeg]\n",
            "[Smile] => [Zmumeg]\n",
            "[Attack] => [Wwet]\n",
            "[Attack] => [Ẓdem]\n",
            "[Cheers] => [S tezmertik]\n",
            "[Cheers] => [S tezmertim]\n",
            "[Cheers] => [S tezmertnwen]\n",
            "[Cheers] => [S tezmertnkent]\n",
            "[Eat up] => [Ečč]\n",
            "[Eat up] => [Ččet]\n",
            "[Eat up] => [Ččemt]\n",
            "[Freeze] => [Ur ttḥerrikem ara]\n",
            "[Freeze] => [Ur ttḥerrikemt ara]\n",
            "[Get up] => [Bded ]\n",
            "[Get up] => [Bedd]\n",
            "[Go now] => [Ṛuḥ tura]\n",
            "[Go now] => [Ṛuḥem tura]\n",
            "[Go now] => [Ṛuḥemt tura]\n",
            "[Got it] => [Gziɣ]\n",
            "[Got it] => [Tegziḍ]\n",
            "[He ran] => [Yettazal]\n",
            "[He ran] => [Yuzzel]\n",
            "[Hug me] => [Geriyid iɣallen]\n",
            "[I fell] => [Ɣliɣ]\n",
            "[I know] => [Ẓṛiɣ]\n",
            "[I know] => [Ɛelmeɣ]\n",
            "[I left] => [Ṛuḥeɣ]\n",
            "[I lied] => [Skerkseɣ]\n",
            "[I lied] => [Skaddbeɣ]\n",
            "[I lost] => [Xesreɣ]\n",
            "[I paid] => [Xellseɣ]\n",
            "[I quit] => [Ad ḥebseɣ]\n",
            "[I swim] => [Ttecfeɣ]\n",
            "[I swim] => [Ttɛummuɣ]\n",
            "[Listen] => [Slemd]\n",
            "[Listen] => [Ḥessemd]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8g2f_VdyOWe"
      },
      "source": [
        "model.summary()\n",
        "\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcy-f-U_WD6u",
        "outputId": "66d274b8-d1e0-4dea-be09-4dfb110a570f"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-kab.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 20000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:18000], dataset[2000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-kab-both.pkl')\n",
        "save_clean_data(train, 'english-kab-train.pkl')\n",
        "save_clean_data(test, 'english-kab-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-kab-both.pkl\n",
            "Saved: english-kab-train.pkl\n",
            "Saved: english-kab-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXrh0-4TWJFZ",
        "outputId": "e5c42c6f-671a-4e79-a3a1-17e5dd79ed70"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Kabyle Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Kabyle Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2970\n",
            "English Max Length: 8\n",
            "Kabyle Vocabulary Size: 12230\n",
            "Kabyle Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           3130880   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 2970)           763290    \n",
            "=================================================================\n",
            "Total params: 4,944,794\n",
            "Trainable params: 4,944,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "282/282 - 101s - loss: 3.4227 - val_loss: 2.8562\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.85623, saving model to model.h5\n",
            "Epoch 2/100\n",
            "282/282 - 94s - loss: 2.7819 - val_loss: 2.6694\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.85623 to 2.66942, saving model to model.h5\n",
            "Epoch 3/100\n",
            "282/282 - 92s - loss: 2.6312 - val_loss: 2.5447\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.66942 to 2.54466, saving model to model.h5\n",
            "Epoch 4/100\n",
            "282/282 - 93s - loss: 2.4855 - val_loss: 2.3886\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.54466 to 2.38858, saving model to model.h5\n",
            "Epoch 5/100\n",
            "282/282 - 92s - loss: 2.3373 - val_loss: 2.2493\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.38858 to 2.24931, saving model to model.h5\n",
            "Epoch 6/100\n",
            "282/282 - 93s - loss: 2.1890 - val_loss: 2.0968\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.24931 to 2.09681, saving model to model.h5\n",
            "Epoch 7/100\n",
            "282/282 - 93s - loss: 2.0401 - val_loss: 1.9575\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.09681 to 1.95752, saving model to model.h5\n",
            "Epoch 8/100\n",
            "282/282 - 93s - loss: 1.8880 - val_loss: 1.8122\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.95752 to 1.81219, saving model to model.h5\n",
            "Epoch 9/100\n",
            "282/282 - 94s - loss: 1.7393 - val_loss: 1.6716\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.81219 to 1.67160, saving model to model.h5\n",
            "Epoch 10/100\n",
            "282/282 - 93s - loss: 1.5926 - val_loss: 1.5344\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.67160 to 1.53435, saving model to model.h5\n",
            "Epoch 11/100\n",
            "282/282 - 92s - loss: 1.4510 - val_loss: 1.4074\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.53435 to 1.40737, saving model to model.h5\n",
            "Epoch 12/100\n",
            "282/282 - 92s - loss: 1.3189 - val_loss: 1.2933\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.40737 to 1.29333, saving model to model.h5\n",
            "Epoch 13/100\n",
            "282/282 - 93s - loss: 1.1930 - val_loss: 1.1718\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.29333 to 1.17179, saving model to model.h5\n",
            "Epoch 14/100\n",
            "282/282 - 93s - loss: 1.0759 - val_loss: 1.0681\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.17179 to 1.06814, saving model to model.h5\n",
            "Epoch 15/100\n",
            "282/282 - 92s - loss: 0.9694 - val_loss: 0.9822\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.06814 to 0.98218, saving model to model.h5\n",
            "Epoch 16/100\n",
            "282/282 - 95s - loss: 0.8759 - val_loss: 0.9054\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.98218 to 0.90544, saving model to model.h5\n",
            "Epoch 17/100\n",
            "282/282 - 92s - loss: 0.7872 - val_loss: 0.8270\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.90544 to 0.82705, saving model to model.h5\n",
            "Epoch 18/100\n",
            "282/282 - 93s - loss: 0.7076 - val_loss: 0.7606\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.82705 to 0.76056, saving model to model.h5\n",
            "Epoch 19/100\n",
            "282/282 - 92s - loss: 0.6340 - val_loss: 0.7011\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.76056 to 0.70109, saving model to model.h5\n",
            "Epoch 20/100\n",
            "282/282 - 93s - loss: 0.5715 - val_loss: 0.6506\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.70109 to 0.65060, saving model to model.h5\n",
            "Epoch 21/100\n",
            "282/282 - 92s - loss: 0.5128 - val_loss: 0.6021\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.65060 to 0.60215, saving model to model.h5\n",
            "Epoch 22/100\n",
            "282/282 - 93s - loss: 0.4625 - val_loss: 0.5648\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.60215 to 0.56478, saving model to model.h5\n",
            "Epoch 23/100\n",
            "282/282 - 93s - loss: 0.4146 - val_loss: 0.5260\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.56478 to 0.52599, saving model to model.h5\n",
            "Epoch 24/100\n",
            "282/282 - 93s - loss: 0.3715 - val_loss: 0.4899\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.52599 to 0.48987, saving model to model.h5\n",
            "Epoch 25/100\n",
            "282/282 - 92s - loss: 0.3342 - val_loss: 0.4650\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.48987 to 0.46496, saving model to model.h5\n",
            "Epoch 26/100\n",
            "282/282 - 93s - loss: 0.3012 - val_loss: 0.4367\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.46496 to 0.43666, saving model to model.h5\n",
            "Epoch 27/100\n",
            "282/282 - 94s - loss: 0.2693 - val_loss: 0.4140\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.43666 to 0.41395, saving model to model.h5\n",
            "Epoch 28/100\n",
            "282/282 - 92s - loss: 0.2423 - val_loss: 0.3975\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.41395 to 0.39746, saving model to model.h5\n",
            "Epoch 29/100\n",
            "282/282 - 93s - loss: 0.2193 - val_loss: 0.3762\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.39746 to 0.37624, saving model to model.h5\n",
            "Epoch 30/100\n",
            "282/282 - 93s - loss: 0.1982 - val_loss: 0.3600\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.37624 to 0.36001, saving model to model.h5\n",
            "Epoch 31/100\n",
            "282/282 - 93s - loss: 0.1786 - val_loss: 0.3504\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.36001 to 0.35044, saving model to model.h5\n",
            "Epoch 32/100\n",
            "282/282 - 92s - loss: 0.1612 - val_loss: 0.3376\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.35044 to 0.33762, saving model to model.h5\n",
            "Epoch 33/100\n",
            "282/282 - 92s - loss: 0.1465 - val_loss: 0.3277\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.33762 to 0.32771, saving model to model.h5\n",
            "Epoch 34/100\n",
            "282/282 - 93s - loss: 0.1330 - val_loss: 0.3231\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.32771 to 0.32309, saving model to model.h5\n",
            "Epoch 35/100\n",
            "282/282 - 92s - loss: 0.1240 - val_loss: 0.3113\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.32309 to 0.31129, saving model to model.h5\n",
            "Epoch 36/100\n",
            "282/282 - 92s - loss: 0.1136 - val_loss: 0.3036\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.31129 to 0.30358, saving model to model.h5\n",
            "Epoch 37/100\n",
            "282/282 - 93s - loss: 0.1042 - val_loss: 0.2988\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.30358 to 0.29878, saving model to model.h5\n",
            "Epoch 38/100\n",
            "282/282 - 93s - loss: 0.0947 - val_loss: 0.2936\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.29878 to 0.29357, saving model to model.h5\n",
            "Epoch 39/100\n",
            "282/282 - 94s - loss: 0.0870 - val_loss: 0.2885\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.29357 to 0.28851, saving model to model.h5\n",
            "Epoch 40/100\n",
            "282/282 - 94s - loss: 0.0814 - val_loss: 0.2883\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.28851 to 0.28831, saving model to model.h5\n",
            "Epoch 41/100\n",
            "282/282 - 93s - loss: 0.0773 - val_loss: 0.2828\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.28831 to 0.28278, saving model to model.h5\n",
            "Epoch 42/100\n",
            "282/282 - 92s - loss: 0.0720 - val_loss: 0.2797\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.28278 to 0.27973, saving model to model.h5\n",
            "Epoch 43/100\n",
            "282/282 - 92s - loss: 0.0678 - val_loss: 0.2796\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.27973 to 0.27959, saving model to model.h5\n",
            "Epoch 44/100\n",
            "282/282 - 92s - loss: 0.0645 - val_loss: 0.2785\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.27959 to 0.27855, saving model to model.h5\n",
            "Epoch 45/100\n",
            "282/282 - 92s - loss: 0.0629 - val_loss: 0.2759\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.27855 to 0.27593, saving model to model.h5\n",
            "Epoch 46/100\n",
            "282/282 - 92s - loss: 0.0593 - val_loss: 0.2765\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27593\n",
            "Epoch 47/100\n",
            "282/282 - 93s - loss: 0.0569 - val_loss: 0.2734\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.27593 to 0.27343, saving model to model.h5\n",
            "Epoch 48/100\n",
            "282/282 - 92s - loss: 0.0536 - val_loss: 0.2735\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27343\n",
            "Epoch 49/100\n",
            "282/282 - 92s - loss: 0.0518 - val_loss: 0.2719\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.27343 to 0.27193, saving model to model.h5\n",
            "Epoch 50/100\n",
            "282/282 - 92s - loss: 0.0512 - val_loss: 0.2734\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27193\n",
            "Epoch 51/100\n",
            "282/282 - 93s - loss: 0.0481 - val_loss: 0.2715\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.27193 to 0.27148, saving model to model.h5\n",
            "Epoch 52/100\n",
            "282/282 - 92s - loss: 0.0475 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.27148\n",
            "Epoch 53/100\n",
            "282/282 - 93s - loss: 0.0469 - val_loss: 0.2716\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.27148\n",
            "Epoch 54/100\n",
            "282/282 - 93s - loss: 0.0461 - val_loss: 0.2755\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.27148\n",
            "Epoch 55/100\n",
            "282/282 - 94s - loss: 0.0476 - val_loss: 0.2753\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.27148\n",
            "Epoch 56/100\n",
            "282/282 - 92s - loss: 0.0520 - val_loss: 0.2785\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.27148\n",
            "Epoch 57/100\n",
            "282/282 - 92s - loss: 0.0500 - val_loss: 0.2777\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.27148\n",
            "Epoch 58/100\n",
            "282/282 - 94s - loss: 0.0456 - val_loss: 0.2740\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.27148\n",
            "Epoch 59/100\n",
            "282/282 - 93s - loss: 0.0422 - val_loss: 0.2694\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.27148 to 0.26944, saving model to model.h5\n",
            "Epoch 60/100\n",
            "282/282 - 93s - loss: 0.0392 - val_loss: 0.2684\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.26944 to 0.26836, saving model to model.h5\n",
            "Epoch 61/100\n",
            "282/282 - 92s - loss: 0.0362 - val_loss: 0.2695\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.26836\n",
            "Epoch 62/100\n",
            "282/282 - 94s - loss: 0.0351 - val_loss: 0.2678\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.26836 to 0.26784, saving model to model.h5\n",
            "Epoch 63/100\n",
            "282/282 - 93s - loss: 0.0354 - val_loss: 0.2687\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.26784\n",
            "Epoch 64/100\n",
            "282/282 - 92s - loss: 0.0348 - val_loss: 0.2699\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.26784\n",
            "Epoch 65/100\n",
            "282/282 - 92s - loss: 0.0346 - val_loss: 0.2708\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.26784\n",
            "Epoch 66/100\n",
            "282/282 - 92s - loss: 0.0383 - val_loss: 0.2842\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.26784\n",
            "Epoch 67/100\n",
            "282/282 - 91s - loss: 0.0610 - val_loss: 0.2943\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.26784\n",
            "Epoch 68/100\n",
            "282/282 - 91s - loss: 0.0553 - val_loss: 0.2838\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.26784\n",
            "Epoch 69/100\n",
            "282/282 - 92s - loss: 0.0436 - val_loss: 0.2743\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.26784\n",
            "Epoch 70/100\n",
            "282/282 - 91s - loss: 0.0357 - val_loss: 0.2734\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.26784\n",
            "Epoch 71/100\n",
            "282/282 - 91s - loss: 0.0328 - val_loss: 0.2714\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.26784\n",
            "Epoch 72/100\n",
            "282/282 - 93s - loss: 0.0315 - val_loss: 0.2715\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.26784\n",
            "Epoch 73/100\n",
            "282/282 - 92s - loss: 0.0312 - val_loss: 0.2705\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.26784\n",
            "Epoch 74/100\n",
            "282/282 - 92s - loss: 0.0317 - val_loss: 0.2720\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.26784\n",
            "Epoch 75/100\n",
            "282/282 - 92s - loss: 0.0318 - val_loss: 0.2718\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.26784\n",
            "Epoch 76/100\n",
            "282/282 - 95s - loss: 0.0323 - val_loss: 0.2742\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.26784\n",
            "Epoch 77/100\n",
            "282/282 - 92s - loss: 0.0333 - val_loss: 0.2734\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.26784\n",
            "Epoch 78/100\n",
            "282/282 - 93s - loss: 0.0334 - val_loss: 0.2770\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.26784\n",
            "Epoch 79/100\n",
            "282/282 - 93s - loss: 0.0391 - val_loss: 0.2904\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.26784\n",
            "Epoch 80/100\n",
            "282/282 - 94s - loss: 0.0515 - val_loss: 0.2889\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.26784\n",
            "Epoch 81/100\n",
            "282/282 - 92s - loss: 0.0454 - val_loss: 0.2814\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.26784\n",
            "Epoch 82/100\n",
            "282/282 - 92s - loss: 0.0377 - val_loss: 0.2782\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.26784\n",
            "Epoch 83/100\n",
            "282/282 - 91s - loss: 0.0331 - val_loss: 0.2767\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.26784\n",
            "Epoch 84/100\n",
            "282/282 - 92s - loss: 0.0310 - val_loss: 0.2751\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.26784\n",
            "Epoch 85/100\n",
            "282/282 - 92s - loss: 0.0305 - val_loss: 0.2754\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.26784\n",
            "Epoch 86/100\n",
            "282/282 - 93s - loss: 0.0301 - val_loss: 0.2751\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.26784\n",
            "Epoch 87/100\n",
            "282/282 - 92s - loss: 0.0299 - val_loss: 0.2758\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.26784\n",
            "Epoch 88/100\n",
            "282/282 - 92s - loss: 0.0300 - val_loss: 0.2748\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.26784\n",
            "Epoch 89/100\n",
            "282/282 - 91s - loss: 0.0302 - val_loss: 0.2787\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.26784\n",
            "Epoch 90/100\n",
            "282/282 - 92s - loss: 0.0313 - val_loss: 0.2771\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.26784\n",
            "Epoch 91/100\n",
            "282/282 - 91s - loss: 0.0312 - val_loss: 0.2787\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.26784\n",
            "Epoch 92/100\n",
            "282/282 - 92s - loss: 0.0317 - val_loss: 0.2802\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.26784\n",
            "Epoch 93/100\n",
            "282/282 - 92s - loss: 0.0418 - val_loss: 0.3025\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.26784\n",
            "Epoch 94/100\n",
            "282/282 - 92s - loss: 0.0535 - val_loss: 0.2913\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.26784\n",
            "Epoch 95/100\n",
            "282/282 - 91s - loss: 0.0402 - val_loss: 0.2844\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.26784\n",
            "Epoch 96/100\n",
            "282/282 - 92s - loss: 0.0332 - val_loss: 0.2782\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.26784\n",
            "Epoch 97/100\n",
            "282/282 - 92s - loss: 0.0301 - val_loss: 0.2788\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.26784\n",
            "Epoch 98/100\n",
            "282/282 - 92s - loss: 0.0287 - val_loss: 0.2776\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.26784\n",
            "Epoch 99/100\n",
            "282/282 - 91s - loss: 0.0284 - val_loss: 0.2781\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.26784\n",
            "Epoch 100/100\n",
            "282/282 - 92s - loss: 0.0286 - val_loss: 0.2773\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.26784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f32126ca990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbvcZWZEabsq",
        "outputId": "103901b8-69cc-470a-f8fa-33c3305394d6"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Kabyle Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Kabyle Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 2970\n",
            "English Max Length: 8\n",
            "Kabyle Vocabulary Size: 12230\n",
            "Kabyle Max Length: 11\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 256)           3130880   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 8, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 8, 2970)           763290    \n",
            "=================================================================\n",
            "Total params: 4,944,794\n",
            "Trainable params: 4,944,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "282/282 - 114s - loss: 3.4313 - val_loss: 2.8549\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.85490, saving model to model.h5\n",
            "Epoch 2/200\n",
            "282/282 - 106s - loss: 2.7899 - val_loss: 2.6715\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.85490 to 2.67150, saving model to model.h5\n",
            "Epoch 3/200\n",
            "282/282 - 105s - loss: 2.6469 - val_loss: 2.5625\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.67150 to 2.56251, saving model to model.h5\n",
            "Epoch 4/200\n",
            "282/282 - 105s - loss: 2.5194 - val_loss: 2.4235\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.56251 to 2.42352, saving model to model.h5\n",
            "Epoch 5/200\n",
            "282/282 - 105s - loss: 2.3711 - val_loss: 2.2828\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.42352 to 2.28278, saving model to model.h5\n",
            "Epoch 6/200\n",
            "282/282 - 105s - loss: 2.2299 - val_loss: 2.1588\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.28278 to 2.15880, saving model to model.h5\n",
            "Epoch 7/200\n",
            "282/282 - 108s - loss: 2.0824 - val_loss: 1.9997\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.15880 to 1.99968, saving model to model.h5\n",
            "Epoch 8/200\n",
            "282/282 - 108s - loss: 1.9300 - val_loss: 1.8492\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.99968 to 1.84918, saving model to model.h5\n",
            "Epoch 9/200\n",
            "282/282 - 107s - loss: 1.7811 - val_loss: 1.7077\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.84918 to 1.70766, saving model to model.h5\n",
            "Epoch 10/200\n",
            "282/282 - 107s - loss: 1.6364 - val_loss: 1.5758\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.70766 to 1.57582, saving model to model.h5\n",
            "Epoch 11/200\n",
            "282/282 - 107s - loss: 1.4964 - val_loss: 1.4455\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.57582 to 1.44550, saving model to model.h5\n",
            "Epoch 12/200\n",
            "282/282 - 107s - loss: 1.3619 - val_loss: 1.3257\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.44550 to 1.32567, saving model to model.h5\n",
            "Epoch 13/200\n",
            "282/282 - 107s - loss: 1.2358 - val_loss: 1.2150\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.32567 to 1.21503, saving model to model.h5\n",
            "Epoch 14/200\n",
            "282/282 - 107s - loss: 1.1191 - val_loss: 1.1091\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.21503 to 1.10907, saving model to model.h5\n",
            "Epoch 15/200\n",
            "282/282 - 107s - loss: 1.0103 - val_loss: 1.0207\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.10907 to 1.02067, saving model to model.h5\n",
            "Epoch 16/200\n",
            "282/282 - 108s - loss: 0.9114 - val_loss: 0.9306\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.02067 to 0.93063, saving model to model.h5\n",
            "Epoch 17/200\n",
            "282/282 - 108s - loss: 0.8206 - val_loss: 0.8525\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.93063 to 0.85248, saving model to model.h5\n",
            "Epoch 18/200\n",
            "282/282 - 107s - loss: 0.7380 - val_loss: 0.7914\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.85248 to 0.79142, saving model to model.h5\n",
            "Epoch 19/200\n",
            "282/282 - 107s - loss: 0.6622 - val_loss: 0.7241\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.79142 to 0.72407, saving model to model.h5\n",
            "Epoch 20/200\n",
            "282/282 - 110s - loss: 0.5915 - val_loss: 0.6730\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.72407 to 0.67299, saving model to model.h5\n",
            "Epoch 21/200\n",
            "282/282 - 109s - loss: 0.5315 - val_loss: 0.6172\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.67299 to 0.61720, saving model to model.h5\n",
            "Epoch 22/200\n",
            "282/282 - 108s - loss: 0.4768 - val_loss: 0.5734\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.61720 to 0.57345, saving model to model.h5\n",
            "Epoch 23/200\n",
            "282/282 - 109s - loss: 0.4285 - val_loss: 0.5354\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.57345 to 0.53541, saving model to model.h5\n",
            "Epoch 24/200\n",
            "282/282 - 108s - loss: 0.3840 - val_loss: 0.4993\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.53541 to 0.49930, saving model to model.h5\n",
            "Epoch 25/200\n",
            "282/282 - 108s - loss: 0.3443 - val_loss: 0.4682\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.49930 to 0.46823, saving model to model.h5\n",
            "Epoch 26/200\n",
            "282/282 - 107s - loss: 0.3083 - val_loss: 0.4420\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.46823 to 0.44197, saving model to model.h5\n",
            "Epoch 27/200\n",
            "282/282 - 105s - loss: 0.2784 - val_loss: 0.4208\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.44197 to 0.42079, saving model to model.h5\n",
            "Epoch 28/200\n",
            "282/282 - 106s - loss: 0.2496 - val_loss: 0.4013\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.42079 to 0.40132, saving model to model.h5\n",
            "Epoch 29/200\n",
            "282/282 - 105s - loss: 0.2240 - val_loss: 0.3820\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.40132 to 0.38197, saving model to model.h5\n",
            "Epoch 30/200\n",
            "282/282 - 106s - loss: 0.2011 - val_loss: 0.3626\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.38197 to 0.36257, saving model to model.h5\n",
            "Epoch 31/200\n",
            "282/282 - 108s - loss: 0.1825 - val_loss: 0.3499\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.36257 to 0.34988, saving model to model.h5\n",
            "Epoch 32/200\n",
            "282/282 - 108s - loss: 0.1638 - val_loss: 0.3401\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.34988 to 0.34012, saving model to model.h5\n",
            "Epoch 33/200\n",
            "282/282 - 107s - loss: 0.1495 - val_loss: 0.3322\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.34012 to 0.33215, saving model to model.h5\n",
            "Epoch 34/200\n",
            "282/282 - 108s - loss: 0.1374 - val_loss: 0.3219\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.33215 to 0.32192, saving model to model.h5\n",
            "Epoch 35/200\n",
            "282/282 - 108s - loss: 0.1240 - val_loss: 0.3096\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.32192 to 0.30958, saving model to model.h5\n",
            "Epoch 36/200\n",
            "282/282 - 108s - loss: 0.1107 - val_loss: 0.3093\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.30958 to 0.30928, saving model to model.h5\n",
            "Epoch 37/200\n",
            "282/282 - 108s - loss: 0.1021 - val_loss: 0.2964\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.30928 to 0.29636, saving model to model.h5\n",
            "Epoch 38/200\n",
            "282/282 - 108s - loss: 0.0959 - val_loss: 0.2933\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.29636 to 0.29327, saving model to model.h5\n",
            "Epoch 39/200\n",
            "282/282 - 108s - loss: 0.0886 - val_loss: 0.2891\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.29327 to 0.28907, saving model to model.h5\n",
            "Epoch 40/200\n",
            "282/282 - 108s - loss: 0.0816 - val_loss: 0.2866\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.28907 to 0.28658, saving model to model.h5\n",
            "Epoch 41/200\n",
            "282/282 - 108s - loss: 0.0770 - val_loss: 0.2876\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.28658\n",
            "Epoch 42/200\n",
            "282/282 - 108s - loss: 0.0731 - val_loss: 0.2814\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.28658 to 0.28137, saving model to model.h5\n",
            "Epoch 43/200\n",
            "282/282 - 108s - loss: 0.0666 - val_loss: 0.2772\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.28137 to 0.27725, saving model to model.h5\n",
            "Epoch 44/200\n",
            "282/282 - 107s - loss: 0.0619 - val_loss: 0.2752\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.27725 to 0.27515, saving model to model.h5\n",
            "Epoch 45/200\n",
            "282/282 - 108s - loss: 0.0583 - val_loss: 0.2751\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.27515 to 0.27514, saving model to model.h5\n",
            "Epoch 46/200\n",
            "282/282 - 107s - loss: 0.0590 - val_loss: 0.2811\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27514\n",
            "Epoch 47/200\n",
            "282/282 - 108s - loss: 0.0583 - val_loss: 0.2762\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27514\n",
            "Epoch 48/200\n",
            "282/282 - 108s - loss: 0.0577 - val_loss: 0.2787\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27514\n",
            "Epoch 49/200\n",
            "282/282 - 108s - loss: 0.0588 - val_loss: 0.2773\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27514\n",
            "Epoch 50/200\n",
            "282/282 - 108s - loss: 0.0558 - val_loss: 0.2740\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.27514 to 0.27402, saving model to model.h5\n",
            "Epoch 51/200\n",
            "282/282 - 107s - loss: 0.0531 - val_loss: 0.2760\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.27402\n",
            "Epoch 52/200\n",
            "282/282 - 107s - loss: 0.0490 - val_loss: 0.2730\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.27402 to 0.27304, saving model to model.h5\n",
            "Epoch 53/200\n",
            "282/282 - 108s - loss: 0.0443 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.27304 to 0.26910, saving model to model.h5\n",
            "Epoch 54/200\n",
            "282/282 - 107s - loss: 0.0418 - val_loss: 0.2687\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.26910 to 0.26871, saving model to model.h5\n",
            "Epoch 55/200\n",
            "282/282 - 108s - loss: 0.0410 - val_loss: 0.2687\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.26871 to 0.26869, saving model to model.h5\n",
            "Epoch 56/200\n",
            "282/282 - 109s - loss: 0.0415 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.26869\n",
            "Epoch 57/200\n",
            "282/282 - 108s - loss: 0.0426 - val_loss: 0.2751\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.26869\n",
            "Epoch 58/200\n",
            "282/282 - 108s - loss: 0.0433 - val_loss: 0.2761\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.26869\n",
            "Epoch 59/200\n",
            "282/282 - 109s - loss: 0.0444 - val_loss: 0.2792\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.26869\n",
            "Epoch 60/200\n",
            "282/282 - 109s - loss: 0.0475 - val_loss: 0.2809\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.26869\n",
            "Epoch 61/200\n",
            "282/282 - 108s - loss: 0.0470 - val_loss: 0.2819\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.26869\n",
            "Epoch 62/200\n",
            "282/282 - 109s - loss: 0.0468 - val_loss: 0.2772\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.26869\n",
            "Epoch 63/200\n",
            "282/282 - 108s - loss: 0.0416 - val_loss: 0.2733\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.26869\n",
            "Epoch 64/200\n",
            "282/282 - 109s - loss: 0.0377 - val_loss: 0.2698\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.26869\n",
            "Epoch 65/200\n",
            "282/282 - 110s - loss: 0.0346 - val_loss: 0.2692\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.26869\n",
            "Epoch 66/200\n",
            "282/282 - 109s - loss: 0.0333 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.26869\n",
            "Epoch 67/200\n",
            "282/282 - 108s - loss: 0.0334 - val_loss: 0.2694\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.26869\n",
            "Epoch 68/200\n",
            "282/282 - 109s - loss: 0.0337 - val_loss: 0.2719\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.26869\n",
            "Epoch 69/200\n",
            "282/282 - 109s - loss: 0.0336 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.26869\n",
            "Epoch 70/200\n",
            "282/282 - 109s - loss: 0.0358 - val_loss: 0.2806\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.26869\n",
            "Epoch 71/200\n",
            "282/282 - 110s - loss: 0.0474 - val_loss: 0.2949\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.26869\n",
            "Epoch 72/200\n",
            "282/282 - 110s - loss: 0.0544 - val_loss: 0.2850\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.26869\n",
            "Epoch 73/200\n",
            "282/282 - 113s - loss: 0.0452 - val_loss: 0.2763\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.26869\n",
            "Epoch 74/200\n",
            "282/282 - 111s - loss: 0.0363 - val_loss: 0.2713\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.26869\n",
            "Epoch 75/200\n",
            "282/282 - 110s - loss: 0.0325 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.26869\n",
            "Epoch 76/200\n",
            "282/282 - 110s - loss: 0.0310 - val_loss: 0.2718\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.26869\n",
            "Epoch 77/200\n",
            "282/282 - 110s - loss: 0.0309 - val_loss: 0.2718\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.26869\n",
            "Epoch 78/200\n",
            "282/282 - 112s - loss: 0.0310 - val_loss: 0.2736\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.26869\n",
            "Epoch 79/200\n",
            "282/282 - 111s - loss: 0.0309 - val_loss: 0.2744\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.26869\n",
            "Epoch 80/200\n",
            "282/282 - 109s - loss: 0.0315 - val_loss: 0.2752\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.26869\n",
            "Epoch 81/200\n",
            "282/282 - 108s - loss: 0.0362 - val_loss: 0.2854\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.26869\n",
            "Epoch 82/200\n",
            "282/282 - 108s - loss: 0.0438 - val_loss: 0.2857\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.26869\n",
            "Epoch 83/200\n",
            "282/282 - 107s - loss: 0.0453 - val_loss: 0.2837\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.26869\n",
            "Epoch 84/200\n",
            "282/282 - 107s - loss: 0.0388 - val_loss: 0.2810\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.26869\n",
            "Epoch 85/200\n",
            "282/282 - 107s - loss: 0.0344 - val_loss: 0.2780\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.26869\n",
            "Epoch 86/200\n",
            "282/282 - 108s - loss: 0.0318 - val_loss: 0.2766\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.26869\n",
            "Epoch 87/200\n",
            "282/282 - 107s - loss: 0.0307 - val_loss: 0.2760\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.26869\n",
            "Epoch 88/200\n",
            "282/282 - 108s - loss: 0.0304 - val_loss: 0.2763\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.26869\n",
            "Epoch 89/200\n",
            "282/282 - 109s - loss: 0.0300 - val_loss: 0.2770\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.26869\n",
            "Epoch 90/200\n",
            "282/282 - 109s - loss: 0.0301 - val_loss: 0.2779\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.26869\n",
            "Epoch 91/200\n",
            "282/282 - 110s - loss: 0.0305 - val_loss: 0.2768\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.26869\n",
            "Epoch 92/200\n",
            "282/282 - 110s - loss: 0.0306 - val_loss: 0.2784\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.26869\n",
            "Epoch 93/200\n",
            "282/282 - 108s - loss: 0.0308 - val_loss: 0.2796\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.26869\n",
            "Epoch 94/200\n",
            "282/282 - 109s - loss: 0.0314 - val_loss: 0.2799\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.26869\n",
            "Epoch 95/200\n",
            "282/282 - 108s - loss: 0.0326 - val_loss: 0.2836\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.26869\n",
            "Epoch 96/200\n",
            "282/282 - 108s - loss: 0.0356 - val_loss: 0.2885\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.26869\n",
            "Epoch 97/200\n",
            "282/282 - 107s - loss: 0.0461 - val_loss: 0.2896\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.26869\n",
            "Epoch 98/200\n",
            "282/282 - 108s - loss: 0.0410 - val_loss: 0.2848\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.26869\n",
            "Epoch 99/200\n",
            "282/282 - 109s - loss: 0.0340 - val_loss: 0.2795\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.26869\n",
            "Epoch 100/200\n",
            "282/282 - 108s - loss: 0.0301 - val_loss: 0.2774\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.26869\n",
            "Epoch 101/200\n",
            "282/282 - 108s - loss: 0.0290 - val_loss: 0.2776\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.26869\n",
            "Epoch 102/200\n",
            "282/282 - 108s - loss: 0.0283 - val_loss: 0.2783\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.26869\n",
            "Epoch 103/200\n",
            "282/282 - 108s - loss: 0.0279 - val_loss: 0.2789\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.26869\n",
            "Epoch 104/200\n",
            "282/282 - 107s - loss: 0.0281 - val_loss: 0.2783\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.26869\n",
            "Epoch 105/200\n",
            "282/282 - 109s - loss: 0.0284 - val_loss: 0.2796\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.26869\n",
            "Epoch 106/200\n",
            "282/282 - 108s - loss: 0.0286 - val_loss: 0.2794\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.26869\n",
            "Epoch 107/200\n",
            "282/282 - 109s - loss: 0.0292 - val_loss: 0.2819\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.26869\n",
            "Epoch 108/200\n",
            "282/282 - 107s - loss: 0.0299 - val_loss: 0.2862\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.26869\n",
            "Epoch 109/200\n",
            "282/282 - 109s - loss: 0.0351 - val_loss: 0.2946\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.26869\n",
            "Epoch 110/200\n",
            "282/282 - 109s - loss: 0.0437 - val_loss: 0.2923\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.26869\n",
            "Epoch 111/200\n",
            "282/282 - 108s - loss: 0.0379 - val_loss: 0.2848\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.26869\n",
            "Epoch 112/200\n",
            "282/282 - 109s - loss: 0.0316 - val_loss: 0.2812\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.26869\n",
            "Epoch 113/200\n",
            "282/282 - 108s - loss: 0.0290 - val_loss: 0.2790\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.26869\n",
            "Epoch 114/200\n",
            "282/282 - 109s - loss: 0.0277 - val_loss: 0.2794\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.26869\n",
            "Epoch 115/200\n",
            "282/282 - 108s - loss: 0.0277 - val_loss: 0.2788\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.26869\n",
            "Epoch 116/200\n",
            "282/282 - 108s - loss: 0.0273 - val_loss: 0.2792\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.26869\n",
            "Epoch 117/200\n",
            "282/282 - 108s - loss: 0.0273 - val_loss: 0.2812\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.26869\n",
            "Epoch 118/200\n",
            "282/282 - 109s - loss: 0.0274 - val_loss: 0.2802\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.26869\n",
            "Epoch 119/200\n",
            "282/282 - 108s - loss: 0.0280 - val_loss: 0.2815\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.26869\n",
            "Epoch 120/200\n",
            "282/282 - 107s - loss: 0.0284 - val_loss: 0.2810\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.26869\n",
            "Epoch 121/200\n",
            "282/282 - 109s - loss: 0.0287 - val_loss: 0.2825\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.26869\n",
            "Epoch 122/200\n",
            "282/282 - 107s - loss: 0.0330 - val_loss: 0.2981\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.26869\n",
            "Epoch 123/200\n",
            "282/282 - 106s - loss: 0.0444 - val_loss: 0.2954\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.26869\n",
            "Epoch 124/200\n",
            "282/282 - 107s - loss: 0.0389 - val_loss: 0.2878\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.26869\n",
            "Epoch 125/200\n",
            "282/282 - 107s - loss: 0.0310 - val_loss: 0.2843\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.26869\n",
            "Epoch 126/200\n",
            "282/282 - 108s - loss: 0.0280 - val_loss: 0.2832\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.26869\n",
            "Epoch 127/200\n",
            "282/282 - 108s - loss: 0.0270 - val_loss: 0.2824\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.26869\n",
            "Epoch 128/200\n",
            "282/282 - 109s - loss: 0.0267 - val_loss: 0.2821\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.26869\n",
            "Epoch 129/200\n",
            "282/282 - 107s - loss: 0.0267 - val_loss: 0.2822\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.26869\n",
            "Epoch 130/200\n",
            "282/282 - 106s - loss: 0.0268 - val_loss: 0.2813\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.26869\n",
            "Epoch 131/200\n",
            "282/282 - 106s - loss: 0.0269 - val_loss: 0.2841\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.26869\n",
            "Epoch 132/200\n",
            "282/282 - 106s - loss: 0.0267 - val_loss: 0.2840\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.26869\n",
            "Epoch 133/200\n",
            "282/282 - 108s - loss: 0.0269 - val_loss: 0.2843\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.26869\n",
            "Epoch 134/200\n",
            "282/282 - 109s - loss: 0.0273 - val_loss: 0.2846\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.26869\n",
            "Epoch 135/200\n",
            "282/282 - 108s - loss: 0.0286 - val_loss: 0.2888\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.26869\n",
            "Epoch 136/200\n",
            "282/282 - 108s - loss: 0.0410 - val_loss: 0.3037\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.26869\n",
            "Epoch 137/200\n",
            "282/282 - 107s - loss: 0.0417 - val_loss: 0.2926\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.26869\n",
            "Epoch 138/200\n",
            "282/282 - 108s - loss: 0.0334 - val_loss: 0.2866\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.26869\n",
            "Epoch 139/200\n",
            "282/282 - 109s - loss: 0.0288 - val_loss: 0.2836\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.26869\n",
            "Epoch 140/200\n",
            "282/282 - 106s - loss: 0.0269 - val_loss: 0.2839\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.26869\n",
            "Epoch 141/200\n",
            "282/282 - 107s - loss: 0.0263 - val_loss: 0.2838\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.26869\n",
            "Epoch 142/200\n",
            "282/282 - 108s - loss: 0.0262 - val_loss: 0.2834\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.26869\n",
            "Epoch 143/200\n",
            "282/282 - 107s - loss: 0.0262 - val_loss: 0.2846\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.26869\n",
            "Epoch 144/200\n",
            "282/282 - 108s - loss: 0.0263 - val_loss: 0.2841\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.26869\n",
            "Epoch 145/200\n",
            "282/282 - 106s - loss: 0.0266 - val_loss: 0.2840\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.26869\n",
            "Epoch 146/200\n",
            "282/282 - 107s - loss: 0.0265 - val_loss: 0.2837\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.26869\n",
            "Epoch 147/200\n",
            "282/282 - 106s - loss: 0.0265 - val_loss: 0.2850\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.26869\n",
            "Epoch 148/200\n",
            "282/282 - 107s - loss: 0.0269 - val_loss: 0.2854\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.26869\n",
            "Epoch 149/200\n",
            "282/282 - 107s - loss: 0.0272 - val_loss: 0.2872\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.26869\n",
            "Epoch 150/200\n",
            "282/282 - 106s - loss: 0.0272 - val_loss: 0.2872\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.26869\n",
            "Epoch 151/200\n",
            "282/282 - 106s - loss: 0.0286 - val_loss: 0.2974\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.26869\n",
            "Epoch 152/200\n",
            "282/282 - 108s - loss: 0.0436 - val_loss: 0.3064\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.26869\n",
            "Epoch 153/200\n",
            "282/282 - 108s - loss: 0.0395 - val_loss: 0.2958\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.26869\n",
            "Epoch 154/200\n",
            "282/282 - 108s - loss: 0.0313 - val_loss: 0.2870\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.26869\n",
            "Epoch 155/200\n",
            "282/282 - 108s - loss: 0.0274 - val_loss: 0.2862\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.26869\n",
            "Epoch 156/200\n",
            "282/282 - 106s - loss: 0.0266 - val_loss: 0.2855\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.26869\n",
            "Epoch 157/200\n",
            "282/282 - 106s - loss: 0.0260 - val_loss: 0.2848\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.26869\n",
            "Epoch 158/200\n",
            "282/282 - 106s - loss: 0.0259 - val_loss: 0.2847\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.26869\n",
            "Epoch 159/200\n",
            "282/282 - 106s - loss: 0.0258 - val_loss: 0.2846\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.26869\n",
            "Epoch 160/200\n",
            "282/282 - 106s - loss: 0.0257 - val_loss: 0.2862\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.26869\n",
            "Epoch 161/200\n",
            "282/282 - 105s - loss: 0.0260 - val_loss: 0.2849\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.26869\n",
            "Epoch 162/200\n",
            "282/282 - 106s - loss: 0.0260 - val_loss: 0.2858\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.26869\n",
            "Epoch 163/200\n",
            "282/282 - 106s - loss: 0.0262 - val_loss: 0.2865\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.26869\n",
            "Epoch 164/200\n",
            "282/282 - 107s - loss: 0.0261 - val_loss: 0.2859\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.26869\n",
            "Epoch 165/200\n",
            "282/282 - 106s - loss: 0.0264 - val_loss: 0.2876\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.26869\n",
            "Epoch 166/200\n",
            "282/282 - 106s - loss: 0.0269 - val_loss: 0.2881\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.26869\n",
            "Epoch 167/200\n",
            "282/282 - 108s - loss: 0.0331 - val_loss: 0.3020\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.26869\n",
            "Epoch 168/200\n",
            "282/282 - 107s - loss: 0.0409 - val_loss: 0.2989\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.26869\n",
            "Epoch 169/200\n",
            "282/282 - 106s - loss: 0.0345 - val_loss: 0.2930\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.26869\n",
            "Epoch 170/200\n",
            "282/282 - 108s - loss: 0.0286 - val_loss: 0.2901\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.26869\n",
            "Epoch 171/200\n",
            "282/282 - 107s - loss: 0.0264 - val_loss: 0.2897\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.26869\n",
            "Epoch 172/200\n",
            "282/282 - 106s - loss: 0.0263 - val_loss: 0.2893\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.26869\n",
            "Epoch 173/200\n",
            "282/282 - 106s - loss: 0.0257 - val_loss: 0.2895\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.26869\n",
            "Epoch 174/200\n",
            "282/282 - 106s - loss: 0.0255 - val_loss: 0.2891\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.26869\n",
            "Epoch 175/200\n",
            "282/282 - 106s - loss: 0.0254 - val_loss: 0.2893\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.26869\n",
            "Epoch 176/200\n",
            "282/282 - 107s - loss: 0.0254 - val_loss: 0.2905\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.26869\n",
            "Epoch 177/200\n",
            "282/282 - 108s - loss: 0.0255 - val_loss: 0.2907\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.26869\n",
            "Epoch 178/200\n",
            "282/282 - 110s - loss: 0.0256 - val_loss: 0.2899\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.26869\n",
            "Epoch 179/200\n",
            "282/282 - 116s - loss: 0.0257 - val_loss: 0.2890\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.26869\n",
            "Epoch 180/200\n",
            "282/282 - 108s - loss: 0.0258 - val_loss: 0.2892\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.26869\n",
            "Epoch 181/200\n",
            "282/282 - 111s - loss: 0.0257 - val_loss: 0.2911\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.26869\n",
            "Epoch 182/200\n",
            "282/282 - 110s - loss: 0.0261 - val_loss: 0.2914\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.26869\n",
            "Epoch 183/200\n",
            "282/282 - 110s - loss: 0.0276 - val_loss: 0.2973\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.26869\n",
            "Epoch 184/200\n",
            "282/282 - 111s - loss: 0.0395 - val_loss: 0.3002\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.26869\n",
            "Epoch 185/200\n",
            "282/282 - 110s - loss: 0.0362 - val_loss: 0.2922\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.26869\n",
            "Epoch 186/200\n",
            "282/282 - 109s - loss: 0.0296 - val_loss: 0.2929\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.26869\n",
            "Epoch 187/200\n",
            "282/282 - 110s - loss: 0.0270 - val_loss: 0.2905\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.26869\n",
            "Epoch 188/200\n",
            "282/282 - 110s - loss: 0.0255 - val_loss: 0.2899\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.26869\n",
            "Epoch 189/200\n",
            "282/282 - 110s - loss: 0.0250 - val_loss: 0.2899\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.26869\n",
            "Epoch 190/200\n",
            "282/282 - 111s - loss: 0.0251 - val_loss: 0.2901\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.26869\n",
            "Epoch 191/200\n",
            "282/282 - 111s - loss: 0.0249 - val_loss: 0.2894\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.26869\n",
            "Epoch 192/200\n",
            "282/282 - 112s - loss: 0.0250 - val_loss: 0.2899\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.26869\n",
            "Epoch 193/200\n",
            "282/282 - 111s - loss: 0.0251 - val_loss: 0.2902\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.26869\n",
            "Epoch 194/200\n",
            "282/282 - 111s - loss: 0.0253 - val_loss: 0.2893\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.26869\n",
            "Epoch 195/200\n",
            "282/282 - 109s - loss: 0.0253 - val_loss: 0.2908\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.26869\n",
            "Epoch 196/200\n",
            "282/282 - 108s - loss: 0.0253 - val_loss: 0.2900\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.26869\n",
            "Epoch 197/200\n",
            "282/282 - 109s - loss: 0.0257 - val_loss: 0.2906\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.26869\n",
            "Epoch 198/200\n",
            "282/282 - 109s - loss: 0.0257 - val_loss: 0.2903\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.26869\n",
            "Epoch 199/200\n",
            "282/282 - 108s - loss: 0.0259 - val_loss: 0.2921\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.26869\n",
            "Epoch 200/200\n",
            "282/282 - 109s - loss: 0.0261 - val_loss: 0.2913\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.26869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0754f727d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYK4I2b-WPTI",
        "outputId": "30d1baae-51f9-4c87-b537-10403292fb4b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('Source=[%s], Target=[%s], Predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('Train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Source=[Tella kra n yiwet i sisawlen i Tom], Target=[Someone called Tom], Predicted=[someone called tom]\n",
            "Source=[Tom iṛuḥ d Mary], Target=[Tom went with Mary], Predicted=[tom went with mary]\n",
            "Source=[Ddmemt wigi], Target=[Take these], Predicted=[take these]\n",
            "Source=[Kcemd di leɛnayak], Target=[Please get in], Predicted=[please get in]\n",
            "Source=[Yella win i yellan deg uxxam], Target=[Anyone home], Predicted=[anyone home]\n",
            "Source=[Yečča Tom s usixef], Target=[Tom ate quickly], Predicted=[tom ate quickly]\n",
            "Source=[Ḥbesit], Target=[Stop it], Predicted=[make it stop]\n",
            "Source=[Ssenqes tazzla], Target=[Slow down], Predicted=[slow down]\n",
            "Source=[D lɛib fellak], Target=[Shame on you], Predicted=[shame on you]\n",
            "Source=[Wali axxaminna], Target=[Look at that house], Predicted=[look at that house]\n",
            "BLEU-1: 0.610482\n",
            "BLEU-2: 0.509154\n",
            "BLEU-3: 0.392476\n",
            "BLEU-4: 0.168266\n",
            "Test\n",
            "Source=[Fakkikent imenɣi], Target=[Stop arguing], Predicted=[stop arguing]\n",
            "Source=[Ttmeslayent tafṛansist], Target=[They spoke French], Predicted=[they spoke french]\n",
            "Source=[Tom yettban d amaglay], Target=[Tom seems sincere], Predicted=[tom seems sincere]\n",
            "Source=[Tom yekreh Mary], Target=[Tom hated Mary], Predicted=[tom hates mary]\n",
            "Source=[Tella Mary ackiţ], Target=[Mary was gorgeous], Predicted=[mary was gorgeous]\n",
            "Source=[Tḥemmelikent Marikan], Target=[America loves you], Predicted=[america loves you]\n",
            "Source=[Kkrent], Target=[They stood], Predicted=[they stood]\n",
            "Source=[Ḍerruntd laksidat], Target=[Accidents happen], Predicted=[accidents happen]\n",
            "Source=[Ḥader], Target=[Take care], Predicted=[take care]\n",
            "Source=[Tezweǧ yids], Target=[She married him], Predicted=[she married him]\n",
            "BLEU-1: 0.572558\n",
            "BLEU-2: 0.474739\n",
            "BLEU-3: 0.366701\n",
            "BLEU-4: 0.155757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwKgZrV679Dr",
        "outputId": "e4f7b1a3-1eef-4032-9969-fcb3854526d4"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('Source=[%s], Target=[%s], Predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('Train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Source=[Meslayem s leɛqel], Target=[Talk slower], Predicted=[talk slower]\n",
            "Source=[N Japu wina], Target=[Is he Japanese], Predicted=[is he japanese]\n",
            "Source=[Ɣef lǧalim i nxeddem], Target=[Were working for you], Predicted=[were working for you]\n",
            "Source=[Sew yidi kra], Target=[Have a drink with me], Predicted=[have a drink with me]\n",
            "Source=[Tixeṛas], Target=[Just leave it], Predicted=[just leave it]\n",
            "Source=[Ḥemmleɣ tikli], Target=[I like walking], Predicted=[i like walking]\n",
            "Source=[Aqlaɣ da merra], Target=[Were all here], Predicted=[were all here]\n",
            "Source=[Ttɛasan meṛṛa], Target=[Everybody waited], Predicted=[everybody waited]\n",
            "Source=[Ḥwaǧeɣ ad ččeɣ tura], Target=[I need to eat now], Predicted=[i need to eat now]\n",
            "Source=[Tḥeṣlemt], Target=[Are you stuck], Predicted=[are you stuck]\n",
            "BLEU-1: 0.655715\n",
            "BLEU-2: 0.580471\n",
            "BLEU-3: 0.512210\n",
            "BLEU-4: 0.339974\n",
            "Test\n",
            "Source=[Sirdemt udmawennkent], Target=[Wash your face], Predicted=[wash your face]\n",
            "Source=[Tom yebɣa mraw d snat n tmellalin], Target=[Tom wants a dozen eggs], Predicted=[tom wants a dozen eggs]\n",
            "Source=[Ḥbes da], Target=[Pull over here], Predicted=[stop here here]\n",
            "Source=[Champagne ttxilem], Target=[Champagne please], Predicted=[champagne please]\n",
            "Source=[Ttuɣ], Target=[Ive forgotten], Predicted=[ive forgotten]\n",
            "Source=[Ḥebsemt da], Target=[Stop here], Predicted=[stop here]\n",
            "Source=[D acu i ntlemdeḍ], Target=[What did you learn], Predicted=[what did you learn]\n",
            "Source=[D tibbuɣyelt daya kan], Target=[This is just stupid], Predicted=[this is just stupid]\n",
            "Source=[Nekkni seg Ustṛalya], Target=[Were from Australia], Predicted=[were from australia]\n",
            "Source=[Ḍsiɣd], Target=[I laughed], Predicted=[i laughed]\n",
            "BLEU-1: 0.601346\n",
            "BLEU-2: 0.527731\n",
            "BLEU-3: 0.468314\n",
            "BLEU-4: 0.306698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLsy3oIvb8zL",
        "outputId": "3638439f-b0af-4b1c-f144-b256dd08643b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('Source=[%s], Target=[%s], Predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('Train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Source=[Ddeqs aya ur ttruɣ ara], Target=[I havent cried in a while], Predicted=[i havent cried in a while]\n",
            "Source=[Tεummeḍ aṭas], Target=[Did you swim much], Predicted=[did you swim much]\n",
            "Source=[Aya mačči d aylak], Target=[This isnt for you], Predicted=[this isnt for you]\n",
            "Source=[Teɛǧebas Boston i Tom], Target=[Tom loved Boston], Predicted=[tom enjoyed boston]\n",
            "Source=[I yelha wassa], Target=[What a lovely day], Predicted=[what a lovely day]\n",
            "Source=[Ur ugadeɣ ara maḍi], Target=[Im not afraid at all], Predicted=[im not afraid at all]\n",
            "Source=[Qebleɣ assumernwen], Target=[I accept your offer], Predicted=[i accept your offer]\n",
            "Source=[Tella tin i yellan deg uxxam], Target=[Anyone home], Predicted=[somebody home in the]\n",
            "Source=[Yexṣeṛ lḥal], Target=[Its cloudy], Predicted=[the weather is terrible]\n",
            "Source=[Ayɣer i tebɣamt akk annecta], Target=[Why do you want all that], Predicted=[why do you want all these]\n",
            "BLEU-1: 0.686607\n",
            "BLEU-2: 0.623809\n",
            "BLEU-3: 0.576840\n",
            "BLEU-4: 0.437631\n",
            "Test\n",
            "Source=[Le teqqaṛeḍ], Target=[Are you studying], Predicted=[are you studying]\n",
            "Source=[Uriɣd tullist assagi], Target=[I wrote an essay today], Predicted=[i wrote an essay today]\n",
            "Source=[Sɛiɣ ccɣel anda nniḍen], Target=[I have business elsewhere], Predicted=[i have business elsewhere]\n",
            "Source=[Ad mtid xedmeɣ], Target=[Im doing it for you], Predicted=[im doing it for you]\n",
            "Source=[Ilaq ad yili kra n yiwen ara tixedmen], Target=[Somebody has to do it], Predicted=[somebody has to do it]\n",
            "Source=[Nettat d tafremlit], Target=[She is a nurse], Predicted=[she is a nurse]\n",
            "Source=[Uɣeɣakd aya], Target=[I brought this for you], Predicted=[i brought this for you]\n",
            "Source=[Eččemt ttxilkent], Target=[Please eat], Predicted=[please eat]\n",
            "Source=[Yettarrayi ttḥulfuɣ s tudert], Target=[It makes me feel alive], Predicted=[it makes me feel alive]\n",
            "Source=[Awimtd lemɛawna], Target=[Bring help], Predicted=[bring help]\n",
            "BLEU-1: 0.650841\n",
            "BLEU-2: 0.587601\n",
            "BLEU-3: 0.544876\n",
            "BLEU-4: 0.409878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6c7RTQCDzYx",
        "outputId": "5687a690-3347-4e39-fccc-014c02f408a3"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('Source=[%s], Target=[%s], Predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('Train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Source=[Ddeqs aya ur ttruɣ ara], Target=[I havent cried in a while], Predicted=[i havent cried in a while]\n",
            "Source=[Tεummeḍ aṭas], Target=[Did you swim much], Predicted=[did you swim much]\n",
            "Source=[Aya mačči d aylak], Target=[This isnt for you], Predicted=[this isnt for you]\n",
            "Source=[Teɛǧebas Boston i Tom], Target=[Tom loved Boston], Predicted=[tom enjoyed boston]\n",
            "Source=[I yelha wassa], Target=[What a lovely day], Predicted=[what a lovely day]\n",
            "Source=[Ur ugadeɣ ara maḍi], Target=[Im not afraid at all], Predicted=[im not afraid at all]\n",
            "Source=[Qebleɣ assumernwen], Target=[I accept your offer], Predicted=[i accept your offer]\n",
            "Source=[Tella tin i yellan deg uxxam], Target=[Anyone home], Predicted=[somebody home in the]\n",
            "Source=[Yexṣeṛ lḥal], Target=[Its cloudy], Predicted=[the weather is terrible]\n",
            "Source=[Ayɣer i tebɣamt akk annecta], Target=[Why do you want all that], Predicted=[why do you want all these]\n",
            "Source=[D tiktim tinna], Target=[Is that your own idea], Predicted=[is that your own idea]\n",
            "Source=[Deg uxxam i tellam], Target=[Are you home], Predicted=[are you home]\n",
            "Source=[Assa yekkat udfel], Target=[Its snowing today], Predicted=[its snowing today]\n",
            "Source=[Beddlemt lqecc], Target=[Get changed], Predicted=[get changed]\n",
            "Source=[Tom ikemmel ccna], Target=[Tom continued singing], Predicted=[tom continued singing]\n",
            "Source=[Tom iṛuḥ ar Ustṛalya], Target=[Tom went to Australia], Predicted=[tom has to to australia]\n",
            "Source=[Iqirrd Tom], Target=[Toms confessed], Predicted=[toms confessed]\n",
            "Source=[Tessen ad tmeslay tajapunit], Target=[She can speak Japanese], Predicted=[she can speak japanese]\n",
            "Source=[Amek akka ur teẓṛim ara], Target=[How can you not know], Predicted=[how can you not know]\n",
            "Source=[Zemreɣ ad kennɣeɣ], Target=[I can kill you], Predicted=[i can kill you]\n",
            "Source=[Ixuṣṣ Tom di tzemmar n tmetti], Target=[Tom lacks social skills], Predicted=[tom lacks social skills]\n",
            "Source=[Ilaq ad tṛuḥeḍ], Target=[Youve got to go], Predicted=[youve got to go]\n",
            "Source=[Yefkayas Tom rrkel i Mary], Target=[Tom kicked Mary], Predicted=[tom kicked mary]\n",
            "Source=[Turiḍ adlis], Target=[Have you written a book], Predicted=[have you written a book]\n",
            "Source=[Steqseɣ Tom], Target=[I asked Tom], Predicted=[i asked tom]\n",
            "Source=[Turaremt atinis], Target=[Did you play tennis], Predicted=[did you play tennis]\n",
            "Source=[Acḥal i trebḥeḍ], Target=[How much did you win], Predicted=[how much did you win]\n",
            "Source=[Ur leḥḥu ara s zzreb], Target=[Dont walk so fast], Predicted=[dont walk so fast]\n",
            "Source=[Tom ibuṣayi], Target=[Tom made me wait], Predicted=[tom made me wait]\n",
            "Source=[Ad akdawiɣ ayekfi], Target=[Ill get you some milk], Predicted=[ill get you some milk]\n",
            "Source=[Rebḥen], Target=[They succeeded], Predicted=[they succeeded]\n",
            "Source=[Anta i yeqqimen], Target=[Who stayed], Predicted=[who stayed]\n",
            "Source=[Teḥwaǧit], Target=[She needs it], Predicted=[she needs it]\n",
            "Source=[Fehmeɣ], Target=[I understand], Predicted=[i understood]\n",
            "Source=[D acu i twalaḍ], Target=[Whatre you thinking], Predicted=[whatre you thinking]\n",
            "Source=[Anzet i yiselmadennwen], Target=[Obey your teachers], Predicted=[obey your teachers]\n",
            "Source=[Tenɣiḍ Tom], Target=[You killed Tom], Predicted=[you killed tom]\n",
            "Source=[Ad yeǧǧ Tom Mary ad texdem ayenni], Target=[Tom will let Mary do that], Predicted=[tom will let mary do that]\n",
            "Source=[Iceyyeɛakn Tom kra n tɣawsa], Target=[Tom sent you something], Predicted=[tom sent you something]\n",
            "Source=[Tebɣam ad ttidtaɣem], Target=[Would you like to buy this], Predicted=[would you like to buy this]\n",
            "Source=[Yesɛa Tom yiwen i kečč], Target=[Tom has one for you], Predicted=[tom has one for you]\n",
            "Source=[Nefkayaktid yakan], Target=[We already gave it to you], Predicted=[we already gave it to you]\n",
            "Source=[Ur bɣiɣ ara ad zegleɣ aya], Target=[I dont want to miss that], Predicted=[i dont want to miss that]\n",
            "Source=[Ḥwaǧeɣ cwiṭ n leḥnana], Target=[I need a little affection], Predicted=[i need a little affection]\n",
            "Source=[Tesɛa leɛqel mliḥ], Target=[She is very wise], Predicted=[she is very wise]\n",
            "Source=[Kulleci yettband d alexxax], Target=[Everything looks dirty], Predicted=[everything looks dirty]\n",
            "Source=[Ad dasaɣ ad kenɛiwneɣ], Target=[Ill come help you], Predicted=[ill come help you]\n",
            "Source=[Tǧehdeḍ], Target=[Are you tough], Predicted=[are you tough]\n",
            "Source=[Snuzgumeɣ fellasen], Target=[Im worried about them], Predicted=[im worried about them]\n",
            "Source=[Iyyawet ɣer texxamtiw], Target=[Come into my room], Predicted=[come into my room]\n",
            "Source=[Ɣret adlisa], Target=[Read this book], Predicted=[read this book]\n",
            "Source=[Ttwaḥeṛṣeɣ mliḥ], Target=[Im fairly busy], Predicted=[im fairly busy]\n",
            "Source=[Ad nemẓeṛ ɣef 230], Target=[Well see you at 230], Predicted=[well see you at 230]\n",
            "Source=[Ur heddṛemt ara], Target=[Dont talk], Predicted=[dont talk]\n",
            "Source=[Ɛni yella yewɛeṛ fellakent], Target=[Was it difficult for you], Predicted=[was it difficult for you]\n",
            "Source=[Cennun yefṛax], Target=[Birds sing], Predicted=[birds sing]\n",
            "Source=[Get ayen tebɣam], Target=[Do as you want], Predicted=[do as you want]\n",
            "Source=[Iniyid amek lliɣ da], Target=[Tell me why Im here], Predicted=[tell me why im here]\n",
            "Source=[Zmreɣ ad drreɣ tawlaft], Target=[Can I have the picture back], Predicted=[can i have the picture back]\n",
            "Source=[Melmi i txeddmem], Target=[When do you work], Predicted=[when do you work]\n",
            "Source=[Ad iliɣ din ɣef lǧalnwen], Target=[Ill be there for you], Predicted=[ill be there for you]\n",
            "Source=[Ɣemzeɣ], Target=[I winked], Predicted=[i winked]\n",
            "Source=[Tom isɛa aṭas n yilellucen], Target=[Tom has many toys], Predicted=[tom has many toys]\n",
            "Source=[Tom ilemmed Tafṛansist], Target=[Tom studies French], Predicted=[tom studies french]\n",
            "Source=[Teɣṛam ta], Target=[Have you read this], Predicted=[have you read this]\n",
            "Source=[Yuɣal Tom ur ittkeyyif ara], Target=[Tom quit smoking], Predicted=[tom quit smoking]\n",
            "Source=[D ayen ibanen belli izmer ad nexdem annecta], Target=[Were sure we can do that], Predicted=[were sure we can do that]\n",
            "Source=[Ɛni tluɛam], Target=[Did you call], Predicted=[did you call]\n",
            "Source=[Zemreɣ ad azzleɣ], Target=[I can run], Predicted=[i can run]\n",
            "Source=[Tom iṛuḥ deg umḍiq n Mary], Target=[Tom went instead of Mary], Predicted=[tom went instead of mary]\n",
            "Source=[Xeẓṛet kan tugnaa], Target=[Look at this picture], Predicted=[look at this picture]\n",
            "Source=[Tom yemmut yak], Target=[Tom is dead isnt he], Predicted=[tom is dead isnt he]\n",
            "Source=[Ɛni thelkeḍ], Target=[Are you sick], Predicted=[are you sick]\n",
            "Source=[Meqqeṛ], Target=[He is old], Predicted=[he is old]\n",
            "Source=[Beṛkakem asuter n ssmaḥ], Target=[Stop apologizing], Predicted=[stop apologizing]\n",
            "Source=[Anwa i tyesɛan], Target=[Who has it], Predicted=[who has it]\n",
            "Source=[Ilaq ad teddum yidi], Target=[You must come with me], Predicted=[you must come with me]\n",
            "Source=[D acu i tttḥesbemt], Target=[What do you think this is], Predicted=[what do you think this is]\n",
            "Source=[Ttettbinemtd mecɣulit], Target=[You seem busy], Predicted=[you seem busy]\n",
            "Source=[D Tom akk i iḍelmen], Target=[Its all Toms fault], Predicted=[its all toms fault]\n",
            "Source=[Yusad Tom], Target=[Tom came], Predicted=[tom came]\n",
            "Source=[Afemd amcicnni], Target=[Find the cat], Predicted=[find the cat]\n",
            "Source=[Nezdeɣ deg Boston], Target=[We live in Boston], Predicted=[we live in boston]\n",
            "Source=[Muddiyid tin yemgaraden], Target=[Give me a different one], Predicted=[give me a different one]\n",
            "Source=[Bɣiɣ aṭas n yedrimen], Target=[I want more money], Predicted=[i want more money]\n",
            "Source=[I ma tella tin i kidiẓṛan], Target=[What if somebody sees you], Predicted=[what if somebody sees you]\n",
            "Source=[Tom isbadu akalasa], Target=[Tom set this record], Predicted=[tom set this record]\n",
            "Source=[Rremiyid s wawal], Target=[Answer me], Predicted=[answer me]\n",
            "Source=[Nella nettnadi fellam], Target=[We were looking for you], Predicted=[weve been looking for you]\n",
            "Source=[Dayen tewweḍd nnubaw], Target=[My turn finally came], Predicted=[my turn finally came]\n",
            "Source=[Ad sssiwleɣ i Tom i lmendadik], Target=[Ill call Tom for you], Predicted=[ill call tom for you]\n",
            "Source=[Irbeḥ Tom], Target=[Tom won], Predicted=[tom won]\n",
            "Source=[D atmaten], Target=[Theyre brothers], Predicted=[theyre brothers]\n",
            "Source=[Teɣṛam ta], Target=[Did you read this], Predicted=[have you read this]\n",
            "Source=[Anda i ilaq ad ttnesres], Target=[Where should we put it], Predicted=[where should we put it]\n",
            "Source=[Ad mnedɛu s lxiṛ], Target=[Well pray for you], Predicted=[well pray for you]\n",
            "Source=[Ad stiniḍ gneɣ], Target=[It seems I was sleeping], Predicted=[it seems i was sleeping]\n",
            "Source=[Tecbeḥ yemma], Target=[My mother is beautiful], Predicted=[my mother is beautiful]\n",
            "Source=[Werǧin ad msefhamen], Target=[They will never agree], Predicted=[they will never agree]\n",
            "Source=[Ɛni ɣelṭeɣ], Target=[Am I wrong], Predicted=[am i wrong]\n",
            "BLEU-1: 0.686607\n",
            "BLEU-2: 0.623809\n",
            "BLEU-3: 0.576840\n",
            "BLEU-4: 0.437631\n",
            "Test\n",
            "Source=[Le teqqaṛeḍ], Target=[Are you studying], Predicted=[are you studying]\n",
            "Source=[Uriɣd tullist assagi], Target=[I wrote an essay today], Predicted=[i wrote an essay today]\n",
            "Source=[Sɛiɣ ccɣel anda nniḍen], Target=[I have business elsewhere], Predicted=[i have business elsewhere]\n",
            "Source=[Ad mtid xedmeɣ], Target=[Im doing it for you], Predicted=[im doing it for you]\n",
            "Source=[Ilaq ad yili kra n yiwen ara tixedmen], Target=[Somebody has to do it], Predicted=[somebody has to do it]\n",
            "Source=[Nettat d tafremlit], Target=[She is a nurse], Predicted=[she is a nurse]\n",
            "Source=[Uɣeɣakd aya], Target=[I brought this for you], Predicted=[i brought this for you]\n",
            "Source=[Eččemt ttxilkent], Target=[Please eat], Predicted=[please eat]\n",
            "Source=[Yettarrayi ttḥulfuɣ s tudert], Target=[It makes me feel alive], Predicted=[it makes me feel alive]\n",
            "Source=[Awimtd lemɛawna], Target=[Bring help], Predicted=[bring help]\n",
            "Source=[Yeɣṛayas Tom i Mary], Target=[Tom called Mary], Predicted=[tom contacted mary]\n",
            "Source=[Tɣuṛṛem tamurtnwen], Target=[You betrayed your country], Predicted=[you betrayed your country]\n",
            "Source=[Susem], Target=[Keep quiet], Predicted=[shut quiet]\n",
            "Source=[D aqcic n ccuq], Target=[He is a spoiled child], Predicted=[he is a spoiled child]\n",
            "Source=[Ayen i tḥemmlem Boston], Target=[Why do you like Boston], Predicted=[why do you like boston]\n",
            "Source=[Uḥwaǧeɣ kra n ḥedd ara yiiɛawnen], Target=[I need somebody to help me], Predicted=[i need someone to help me]\n",
            "Source=[Ur ttɣimit ara ɣef usenɣay], Target=[Dont sit down on the sofa], Predicted=[dont sit down on the sofa]\n",
            "Source=[Lkaɣeḍ yettṛuɣ s sshala], Target=[Paper burns easily], Predicted=[paper burns easily]\n",
            "Source=[Tekkat lehwa], Target=[It rained], Predicted=[it rained]\n",
            "Source=[Ayen atqelleq], Target=[Why bother], Predicted=[why bother]\n",
            "Source=[Bɣiɣ ad yili kra n yiwen d wi ara heḍṛeɣ], Target=[I want someone to talk to], Predicted=[i want someone to talk to]\n",
            "Source=[Ad kdsewweɣ], Target=[Ill cook for you], Predicted=[ill cook for you]\n",
            "Source=[Mazaliyi kerheɣk], Target=[I still hate you], Predicted=[i still hate you]\n",
            "Source=[Di leɛnayam steqsi walebɛaḍnniḍen], Target=[Please ask someone else], Predicted=[please ask someone else]\n",
            "Source=[Yessewham waya ɣer ɣurwen], Target=[Was that weird for you], Predicted=[was that weird for you]\n",
            "Source=[Tḥemmel tigaṭuyin], Target=[She loves cake], Predicted=[she loves cake]\n",
            "Source=[D acu i dastettakkemt i weqziḥnkent ad tyečč ], Target=[What do you feed your dog], Predicted=[what do you feed your dog]\n",
            "Source=[Eǧǧ Tom i yimanis], Target=[Leave Tom alone], Predicted=[leave tom alone]\n",
            "Source=[Yella kra i dyeḍṛan], Target=[Has something happened], Predicted=[did something happen]\n",
            "Source=[Tufamt akk imannkent], Target=[Are you fully recovered], Predicted=[are you fully recovered]\n",
            "Source=[Ḥemmleɣ ad cfeɣ], Target=[I like swimming], Predicted=[i like swimming]\n",
            "Source=[Tfeṛḥemt meṛṛa], Target=[Youre all happy], Predicted=[youre all happy]\n",
            "Source=[Sseqsi Tom ma tessen ad tεum Mary], Target=[Ask Tom if Mary can swim], Predicted=[ask tom if mary can swim]\n",
            "Source=[Iɛawed Tom axeddim], Target=[Tom changed jobs], Predicted=[tom changed jobs]\n",
            "Source=[Sewweɣakd imensi], Target=[I cooked dinner for you], Predicted=[i cooked dinner for you]\n",
            "Source=[Sexsi tiliẓṛi], Target=[Turn off the TV], Predicted=[turn off the tv]\n",
            "Source=[Tigi nkent], Target=[Theyre yours], Predicted=[theyre yours]\n",
            "Source=[Llan tetten], Target=[They were eating], Predicted=[they were eating]\n",
            "Source=[Zemreɣ ad amslemdeɣ amek ad tessewweḍ], Target=[I can teach you how to cook], Predicted=[i can teach you how to cook]\n",
            "Source=[Heddṛen akk medden], Target=[Everyone gossips], Predicted=[everyone gossips]\n",
            "Source=[Ttṛajunt meṛṛa], Target=[Everyone waited], Predicted=[everyone waited]\n",
            "Source=[Ur walaɣ yiwen], Target=[I didnt see anyone], Predicted=[i didnt see anyone]\n",
            "Source=[Errit d abesṭuḥ], Target=[Make it smaller], Predicted=[make it smaller]\n",
            "Source=[Muqlet amek ara tezhum iḍagi], Target=[Try to have fun tonight], Predicted=[try to have fun tonight]\n",
            "Source=[Rnumt takukit nniḍen], Target=[Have another cookie], Predicted=[have another cookie]\n",
            "Source=[Ad dyaweḍ wassik], Target=[Your time will come], Predicted=[your time will come]\n",
            "Source=[Nekkni d imeddukkal], Target=[Are we friends], Predicted=[are we friends]\n",
            "Source=[Ittɛedday wakud], Target=[Time flies], Predicted=[time flies]\n",
            "Source=[Sarameɣam ussan n usteɛfu yelhan], Target=[Happy holidays], Predicted=[happy holidays]\n",
            "Source=[Maci d amcic d aydi], Target=[Its not a cat Its a dog], Predicted=[its not a cat its a dog]\n",
            "Source=[D acu i kidyewwin ɣer Boston], Target=[What brings you to Boston], Predicted=[what brings you to boston]\n",
            "Source=[Lliɣ heggaɣ i lmendadik], Target=[I was ready for you], Predicted=[i was ready for you]\n",
            "Source=[Ad nemẓeṛ tikkeltnniḍen], Target=[See you later], Predicted=[see you later]\n",
            "Source=[I yecbeḥ], Target=[How beautiful], Predicted=[how cute]\n",
            "Source=[Tzemreḍ ad tkemmleḍ], Target=[Can you continue], Predicted=[you you continue]\n",
            "Source=[Ilaq ad tesgunfumt], Target=[You need to relax], Predicted=[you must rest]\n",
            "Source=[Qqim g umkanim], Target=[Stay put], Predicted=[stay put]\n",
            "Source=[Ɛni d tidet], Target=[Is it true], Predicted=[is it true]\n",
            "Source=[Ayaagi akk ittbaniyid d lewhayem], Target=[All this is strange to me], Predicted=[all this is strange to me]\n",
            "Source=[Tom yettban ɣurs aɣilif], Target=[Tom looks bored], Predicted=[tom looks bored]\n",
            "Source=[Tella tin i yettnezzihen], Target=[Someone is watching], Predicted=[someone is watching]\n",
            "Source=[Melmi ara kentisuɛed lḥal], Target=[When is good for you], Predicted=[when is good for you]\n",
            "Source=[D ayen yuɛṛen], Target=[Its dangerous], Predicted=[its dangerous]\n",
            "Source=[Kcemd ɣer dagi Tom], Target=[Get in here Tom], Predicted=[get in here tom]\n",
            "Source=[Tom d amedyaz], Target=[Tom is a poet], Predicted=[tom is a poet]\n",
            "Source=[Tsettem akken iqwem], Target=[Are you eating right], Predicted=[are you eating right]\n",
            "Source=[D acu yeḍran d Tom], Target=[Whats become of Tom], Predicted=[whats become of tom]\n",
            "Source=[Anɛam], Target=[Pardon me], Predicted=[pardon me]\n",
            "Source=[Iɣellid ugeffur], Target=[It is raining], Predicted=[it is raining]\n",
            "Source=[Wagi werǧin i dyeḍri], Target=[This is unprecedented], Predicted=[this is unprecedented]\n",
            "Source=[Imdanen ad ssemḥasen], Target=[Are people listening], Predicted=[are people listening]\n",
            "Source=[Ǧǧetaɣttid], Target=[Leave it up to us], Predicted=[leave it up to us]\n",
            "Source=[Ḥbes ur ggar ara imanik], Target=[Stop meddling], Predicted=[stop meddling]\n",
            "Source=[Sexdameɣt], Target=[I use it], Predicted=[i use it]\n",
            "Source=[Ixeddem Tom imensi], Target=[Tom cooked dinner], Predicted=[tom cooked dinner]\n",
            "Source=[Iskaddebiyid akken ad iffeɣ], Target=[He lied his way out of it], Predicted=[he lied his way out of it]\n",
            "Source=[Initaɣd tidet], Target=[Tell us the truth], Predicted=[tell us the truth]\n",
            "Source=[Izmer ad yidteḥbes kra n yiwet], Target=[Somebody might stop me], Predicted=[somebody might stop me]\n",
            "Source=[La yekkat wedfel deg Boston], Target=[Its snowing in Boston], Predicted=[its snowing in boston]\n",
            "Source=[Yella win i dyeɣṛan], Target=[Somebody called], Predicted=[somebody called]\n",
            "Source=[Tewwid ad tḥadrem], Target=[You must be careful], Predicted=[you must be careful]\n",
            "Source=[Zemreɣ ad nṛuḥeɣ], Target=[Can I come], Predicted=[can i come]\n",
            "Source=[Timlilit deg Boston], Target=[See you in Boston], Predicted=[see you in boston]\n",
            "Source=[Ɣuṛk ad terreẓeḍ], Target=[Never give up], Predicted=[never give up]\n",
            "Source=[Tettferriǧemt tiliẓri yal ass], Target=[Do you watch TV every day], Predicted=[do you watch tv every day]\n",
            "Source=[Sεiɣ igerdan], Target=[I have children], Predicted=[i have children]\n",
            "Source=[Ur ttmesxiret ara felli], Target=[Dont mess with me], Predicted=[dont mess with me]\n",
            "Source=[Jemɛeɣd Tom s axxam], Target=[I took Tom home], Predicted=[i took tom home]\n",
            "Source=[Anda ara tṛuḥeḍ], Target=[Where will you go], Predicted=[where will you go]\n",
            "Source=[Di tagar Tom yemlal akked Mary], Target=[Tom finally met Mary], Predicted=[tom finally met mary]\n",
            "Source=[Jmeεitent], Target=[You keep them], Predicted=[keep them them]\n",
            "Source=[Yella win i mdisawlen], Target=[Did someone contact you], Predicted=[did someone contact you]\n",
            "Source=[Laqeniyi waman], Target=[I need water], Predicted=[i need water]\n",
            "Source=[Ẓriɣkemid d Tom], Target=[I saw you with Tom], Predicted=[i saw you with tom]\n",
            "Source=[Ayɣer i ttidtuɣemt], Target=[Why did you buy it], Predicted=[why did you buy it]\n",
            "Source=[Ḥemmleɣ tamdintiw], Target=[I love my city], Predicted=[i love my city]\n",
            "Source=[Uwiɣawentid kra n wučču], Target=[I brought you some food], Predicted=[i brought you some food]\n",
            "Source=[Siwzelit], Target=[Make it short], Predicted=[make it short]\n",
            "Source=[Ḥezzbemt], Target=[Take precautions], Predicted=[take precautions]\n",
            "Source=[Tewwet lgerra idelli], Target=[It rained yesterday], Predicted=[it rained yesterday]\n",
            "BLEU-1: 0.650841\n",
            "BLEU-2: 0.587601\n",
            "BLEU-3: 0.544876\n",
            "BLEU-4: 0.409878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DWHpAKjX8sQ",
        "outputId": "85e73bc8-7839-4046-a885-81d09bac5e9b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('Source=[%s], Target=[%s], Predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kab-both.pkl')\n",
        "train = load_clean_sentences('english-kab-train.pkl')\n",
        "test = load_clean_sentences('english-kab-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('Train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('Test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Source=[Tibḥirtiw d tamecṭuḥt], Target=[My garden is small], Predicted=[my garden is small]\n",
            "Source=[Ṛeqqeɛ waki], Target=[Fix this], Predicted=[fix this]\n",
            "Source=[Ttuɣ kullec ɣef waya], Target=[I forgot all about that], Predicted=[i forgot all about that]\n",
            "Source=[Tameṭṭut n Tom d mmtismin], Target=[Tom has a jealous wife], Predicted=[tom has a jealous wife]\n",
            "Source=[Ad mdyecnu Tom], Target=[Tom will sing for you], Predicted=[tom will sing for you]\n",
            "Source=[Andat ttbut], Target=[Wheres the proof], Predicted=[wheres the proof]\n",
            "Source=[D acu i nezmer ad kenttid nexdem], Target=[What can we do for you], Predicted=[what can we do for you]\n",
            "Source=[Amek i sqqaṛen i babam], Target=[Whats your dads name], Predicted=[whats your dads name]\n",
            "Source=[Ur ttɛaqabet ara Tom ɣef ayen], Target=[Dont punish Tom for that], Predicted=[dont punish tom for that]\n",
            "Source=[Xedmeɣam kullec], Target=[I did everything for you], Predicted=[i did everything for you]\n",
            "BLEU-1: 0.686006\n",
            "BLEU-2: 0.623655\n",
            "BLEU-3: 0.577418\n",
            "BLEU-4: 0.438545\n",
            "Test\n",
            "Source=[Ssizdeg taxxamtim], Target=[Clean up your room], Predicted=[clean up your room]\n",
            "Source=[Tom yegguraɛd], Target=[Tom burped], Predicted=[tom burped]\n",
            "Source=[Mxellen], Target=[Theyre crazy], Predicted=[theyre crazy]\n",
            "Source=[Tella kra n yiwet i yukren apaspuṛiw], Target=[Someone stole my passport], Predicted=[someone stole my passport]\n",
            "Source=[Acḥal ara tesεeddimt da], Target=[How long will you stay here], Predicted=[how long will you stay here]\n",
            "Source=[D iwessaren], Target=[Theyre old], Predicted=[theyre old]\n",
            "Source=[Sukkd afrannniḍen], Target=[Make another choice], Predicted=[make another choice]\n",
            "Source=[Tuɣ yekkat wedfel], Target=[It was snowing], Predicted=[it was snowing]\n",
            "Source=[Agaṭunkent d aẓidan], Target=[Your cake is delicious], Predicted=[your cake is delicious]\n",
            "Source=[Ilaq ad iyitamneḍ], Target=[You must believe me], Predicted=[you must believe me]\n",
            "BLEU-1: 0.648385\n",
            "BLEU-2: 0.585603\n",
            "BLEU-3: 0.543434\n",
            "BLEU-4: 0.408951\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}